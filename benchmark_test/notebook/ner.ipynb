{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12f227bb",
   "metadata": {},
   "source": [
    " * @ Author: Yohei Ohto\n",
    " * @ Create Time: 2025-11-28 20:10:30\n",
    " * @ Modified time: 2025-11-28 20:15:46\n",
    " * @ Description: 既存MLMへのNER実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21858a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoModel, AutoModelForTokenClassification,\n",
    "                          AutoTokenizer, DataCollatorWithPadding, Trainer, DataCollatorForTokenClassification,\n",
    "                          TrainingArguments)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d7d1b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"YoheiOhto/megatron_test_251215\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd80014",
   "metadata": {},
   "source": [
    "# NCBI Desease Named Entity Recognition Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1961ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ncbi/ncbi_disease\")\n",
    "\n",
    "label_names = [\n",
    "    \"O\", \"B-DISEASE\", \"I-DISEASE\"\n",
    "]\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(label_names)}\n",
    "label2id = {name: i for i, name in enumerate(label_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bb98e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p: tuple):\n",
    "    \"\"\"\n",
    "    NERの評価指標を計算する関数\n",
    "    Args:\n",
    "        p: モデルの予測結果とラベルのタプル (predictions, labels)\n",
    "        label_names (list): ラベル名のリスト\n",
    "    Returns:\n",
    "        dict: precision, recall, f1, accuracyを含む辞書\n",
    "    How to use:\n",
    "        from transformers import Trainer\n",
    "\n",
    "        trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "    \"\"\"\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    seqeval = evaluate.load(\"seqeval\")\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": float(results[\"overall_precision\"]),\n",
    "        \"recall\": float(results[\"overall_recall\"]),\n",
    "        \"f1\": float(results[\"overall_f1\"]),\n",
    "        \"accuracy\": float(results[\"overall_accuracy\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e992921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: YoheiOhto/megatron_test_251215  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test_env/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:1025: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/test_env/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:492: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at YoheiOhto/megatron_test_251215 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 5433/5433 [00:01<00:00, 4769.92 examples/s]\n",
      "Map: 100%|██████████| 924/924 [00:00<00:00, 3667.79 examples/s]\n",
      "Map: 100%|██████████| 941/941 [00:00<00:00, 6125.42 examples/s]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33moy826c60\u001b[0m (\u001b[33m250502_ohto_research\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251215_152413-1940madf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/NCBI-disease/runs/1940madf' target=\"_blank\">megatron_test_251215</a></strong> to <a href='https://wandb.ai/250502_ohto_research/NCBI-disease' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/NCBI-disease' target=\"_blank\">https://wandb.ai/250502_ohto_research/NCBI-disease</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/NCBI-disease/runs/1940madf' target=\"_blank\">https://wandb.ai/250502_ohto_research/NCBI-disease/runs/1940madf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_967970/3415499466.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3400' max='3400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3400/3400 04:56, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.095296</td>\n",
       "      <td>0.566514</td>\n",
       "      <td>0.627700</td>\n",
       "      <td>0.595539</td>\n",
       "      <td>0.967541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.076176</td>\n",
       "      <td>0.594303</td>\n",
       "      <td>0.768742</td>\n",
       "      <td>0.670360</td>\n",
       "      <td>0.975218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.092553</td>\n",
       "      <td>0.653938</td>\n",
       "      <td>0.696315</td>\n",
       "      <td>0.674462</td>\n",
       "      <td>0.976011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>0.100401</td>\n",
       "      <td>0.697561</td>\n",
       "      <td>0.726811</td>\n",
       "      <td>0.711886</td>\n",
       "      <td>0.978055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.112988</td>\n",
       "      <td>0.692671</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.717697</td>\n",
       "      <td>0.978097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.119692</td>\n",
       "      <td>0.692217</td>\n",
       "      <td>0.745870</td>\n",
       "      <td>0.718043</td>\n",
       "      <td>0.978389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.118718</td>\n",
       "      <td>0.685979</td>\n",
       "      <td>0.752224</td>\n",
       "      <td>0.717576</td>\n",
       "      <td>0.978639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.126282</td>\n",
       "      <td>0.689412</td>\n",
       "      <td>0.744600</td>\n",
       "      <td>0.715944</td>\n",
       "      <td>0.978347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.127175</td>\n",
       "      <td>0.692849</td>\n",
       "      <td>0.750953</td>\n",
       "      <td>0.720732</td>\n",
       "      <td>0.978681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.693662</td>\n",
       "      <td>0.750953</td>\n",
       "      <td>0.721171</td>\n",
       "      <td>0.978556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, name in enumerate(models):\n",
    "    print(\"=== Model:\", name, \" Training ===\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(name, use_auth_token=hf_token)\n",
    "    tokenizer.pad_token = \"[PAD]\" \n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "    name, use_auth_token=hf_token,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    tokenized_datasets = dataset.map(\n",
    "        lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "        batched=True\n",
    "    )\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    model_name = name.split(\"/\")[-1]\n",
    "    \n",
    "    wandb.init(\n",
    "    entity=\"250502_ohto_research\",\n",
    "    project=\"NCBI-disease\", name=model_name, \n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 10,\n",
    "        \"dataset\": \"NCBI-Disease\",\n",
    "    })\n",
    "\n",
    "    os.makedirs(f\"../result/ncbi/{model_name}\", exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"../result/ncbi/{model_name}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "    df_final_results = result_output_general(\n",
    "        trainer=trainer,\n",
    "        tokenized_datasets=tokenized_datasets,\n",
    "        tokenizer=tokenizer,\n",
    "        id2label=id2label,\n",
    "        num_samples_to_process=len(tokenized_datasets[\"validation\"]),\n",
    "        output_filename=f'../result/ncbi/{model_name}/results.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a718e85d",
   "metadata": {},
   "source": [
    "# BC5CDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf635c01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 4561/4561 [00:00<00:00, 136104.87 examples/s]\n",
      "Generating validation split: 100%|██████████| 4582/4582 [00:00<00:00, 147095.34 examples/s]\n",
      "Generating test split: 100%|██████████| 4798/4798 [00:00<00:00, 57601.76 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names: ['O', 'B-DISEASE', 'I-DISEASE', 'B-CHEMICAL', 'I-CHEMICAL']\n",
      "ID to Label mapping: {0: 'O', 1: 'B-DISEASE', 2: 'I-DISEASE', 3: 'B-CHEMICAL', 4: 'I-CHEMICAL'}\n",
      "Label to ID mapping: {'O': 0, 'B-DISEASE': 1, 'I-DISEASE': 2, 'B-CHEMICAL': 3, 'I-CHEMICAL': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"omniquad/BC5CDR-IOB\")\n",
    "\n",
    "label_names = [\n",
    "    \"O\", \"B-DISEASE\", \"I-DISEASE\", \"B-CHEMICAL\", \"I-CHEMICAL\"\n",
    "]\n",
    "\n",
    "id2label = {i: name for i, name in enumerate(label_names)}\n",
    "label2id = {name: i for i, name in enumerate(label_names)}\n",
    "\n",
    "print(\"Label names:\", label_names)\n",
    "print(\"ID to Label mapping:\", id2label)\n",
    "print(\"Label to ID mapping:\", label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23846b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 21273.67 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 39388.46 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 35685.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def convert_labels_to_ids(example):\n",
    "    example['ner_tags'] = [label2id[label_str] for label_str in example['ner_tags']]\n",
    "    return example\n",
    "dataset = dataset.map(convert_labels_to_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3cc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 8906.09 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 9494.54 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 9819.01 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▆█▆▇▇████</td></tr><tr><td>eval/f1</td><td>▁▄▅▇▆▇▇████</td></tr><tr><td>eval/loss</td><td>▂▁▂▃▆▆▇████</td></tr><tr><td>eval/precision</td><td>▁▄▆█▅█▇████</td></tr><tr><td>eval/recall</td><td>▁▄▄▇▇▆▇████</td></tr><tr><td>eval/runtime</td><td>▃▄▁█▄▂▃▄▁▂▄</td></tr><tr><td>eval/samples_per_second</td><td>▆▅█▁▅▇▆▅█▆▄</td></tr><tr><td>eval/steps_per_second</td><td>▆▅█▁▅▇▆▅█▆▄</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.98586</td></tr><tr><td>eval/f1</td><td>0.84477</td></tr><tr><td>eval/loss</td><td>0.10147</td></tr><tr><td>eval/precision</td><td>0.82289</td></tr><tr><td>eval/recall</td><td>0.86785</td></tr><tr><td>eval/runtime</td><td>2.7104</td></tr><tr><td>eval/samples_per_second</td><td>340.912</td></tr><tr><td>eval/steps_per_second</td><td>21.399</td></tr><tr><td>test/accuracy</td><td>0.98586</td></tr><tr><td>test/f1</td><td>0.84477</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BioClinical-ModernBERT-base</strong> at: <a href='https://wandb.ai/250502_ohto_research/NCBI-disease/runs/549zi93p' target=\"_blank\">https://wandb.ai/250502_ohto_research/NCBI-disease/runs/549zi93p</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/NCBI-disease' target=\"_blank\">https://wandb.ai/250502_ohto_research/NCBI-disease</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_132705-549zi93p/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_133256-7g84gmps</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/7g84gmps' target=\"_blank\">BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/7g84gmps' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/7g84gmps</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 03:54, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.082857</td>\n",
       "      <td>0.847195</td>\n",
       "      <td>0.894089</td>\n",
       "      <td>0.870011</td>\n",
       "      <td>0.972678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.076229</td>\n",
       "      <td>0.876698</td>\n",
       "      <td>0.901282</td>\n",
       "      <td>0.888820</td>\n",
       "      <td>0.977804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.086503</td>\n",
       "      <td>0.878622</td>\n",
       "      <td>0.903992</td>\n",
       "      <td>0.891127</td>\n",
       "      <td>0.976365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.096066</td>\n",
       "      <td>0.895124</td>\n",
       "      <td>0.912853</td>\n",
       "      <td>0.903902</td>\n",
       "      <td>0.979098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>0.111860</td>\n",
       "      <td>0.863264</td>\n",
       "      <td>0.922026</td>\n",
       "      <td>0.891678</td>\n",
       "      <td>0.975190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.114379</td>\n",
       "      <td>0.888190</td>\n",
       "      <td>0.912540</td>\n",
       "      <td>0.900201</td>\n",
       "      <td>0.977923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.893972</td>\n",
       "      <td>0.910560</td>\n",
       "      <td>0.902190</td>\n",
       "      <td>0.978144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.128511</td>\n",
       "      <td>0.892719</td>\n",
       "      <td>0.912540</td>\n",
       "      <td>0.902521</td>\n",
       "      <td>0.978630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.129583</td>\n",
       "      <td>0.895001</td>\n",
       "      <td>0.912540</td>\n",
       "      <td>0.903685</td>\n",
       "      <td>0.978766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.130246</td>\n",
       "      <td>0.895716</td>\n",
       "      <td>0.913270</td>\n",
       "      <td>0.904408</td>\n",
       "      <td>0.978970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: dmis-lab/biobert-v1.1  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 9359.83 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 10056.76 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 8332.75 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▇▅█▄▇▇▇███</td></tr><tr><td>eval/f1</td><td>▁▅▅█▅▇█████</td></tr><tr><td>eval/loss</td><td>▂▁▂▄▆▆▇████</td></tr><tr><td>eval/precision</td><td>▁▅▆█▃▇█████</td></tr><tr><td>eval/recall</td><td>▁▃▃▆█▆▅▆▆▆▆</td></tr><tr><td>eval/runtime</td><td>▃▂█▃▂▃▁▂▁▃▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▇▁▆▇▆█▇█▆█</td></tr><tr><td>eval/steps_per_second</td><td>▆▇▁▆▇▆█▇█▆█</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97897</td></tr><tr><td>eval/f1</td><td>0.90441</td></tr><tr><td>eval/loss</td><td>0.13025</td></tr><tr><td>eval/precision</td><td>0.89572</td></tr><tr><td>eval/recall</td><td>0.91327</td></tr><tr><td>eval/runtime</td><td>6.5051</td></tr><tr><td>eval/samples_per_second</td><td>704.372</td></tr><tr><td>eval/steps_per_second</td><td>44.119</td></tr><tr><td>test/accuracy</td><td>0.97897</td></tr><tr><td>test/f1</td><td>0.90441</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext</strong> at: <a href='https://wandb.ai/250502_ohto_research/BC5/runs/7g84gmps' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/7g84gmps</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_133256-7g84gmps/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_133713-a9vqksn6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/a9vqksn6' target=\"_blank\">biobert-v1.1</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/a9vqksn6' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/a9vqksn6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 04:38, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.077478</td>\n",
       "      <td>0.853661</td>\n",
       "      <td>0.894600</td>\n",
       "      <td>0.873651</td>\n",
       "      <td>0.973872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.079682</td>\n",
       "      <td>0.867188</td>\n",
       "      <td>0.901272</td>\n",
       "      <td>0.883902</td>\n",
       "      <td>0.975984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.110500</td>\n",
       "      <td>0.094465</td>\n",
       "      <td>0.856060</td>\n",
       "      <td>0.906485</td>\n",
       "      <td>0.880551</td>\n",
       "      <td>0.973285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.099452</td>\n",
       "      <td>0.888456</td>\n",
       "      <td>0.902627</td>\n",
       "      <td>0.895485</td>\n",
       "      <td>0.977313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.118190</td>\n",
       "      <td>0.860665</td>\n",
       "      <td>0.909925</td>\n",
       "      <td>0.884610</td>\n",
       "      <td>0.974835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008600</td>\n",
       "      <td>0.120439</td>\n",
       "      <td>0.886769</td>\n",
       "      <td>0.901376</td>\n",
       "      <td>0.894013</td>\n",
       "      <td>0.977466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.127201</td>\n",
       "      <td>0.887654</td>\n",
       "      <td>0.899500</td>\n",
       "      <td>0.893538</td>\n",
       "      <td>0.976904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.137474</td>\n",
       "      <td>0.879281</td>\n",
       "      <td>0.907423</td>\n",
       "      <td>0.893130</td>\n",
       "      <td>0.976683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.138912</td>\n",
       "      <td>0.877722</td>\n",
       "      <td>0.907736</td>\n",
       "      <td>0.892476</td>\n",
       "      <td>0.976640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.139211</td>\n",
       "      <td>0.883690</td>\n",
       "      <td>0.903774</td>\n",
       "      <td>0.893619</td>\n",
       "      <td>0.976921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: google-bert/bert-base-cased  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 10349.76 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 10691.87 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 8286.66 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▂▆▁█▄█▇▇▇▇▇</td></tr><tr><td>eval/f1</td><td>▁▄▃█▅█▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>▁▁▃▃▆▆▇████</td></tr><tr><td>eval/precision</td><td>▁▄▁█▂██▆▆▇▇</td></tr><tr><td>eval/recall</td><td>▁▄▆▅█▄▃▇▇▅▅</td></tr><tr><td>eval/runtime</td><td>▁▁▁▁▁▄█▃▂▁▄</td></tr><tr><td>eval/samples_per_second</td><td>█████▄▁▆▇█▄</td></tr><tr><td>eval/steps_per_second</td><td>█████▄▁▆▇█▄</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97692</td></tr><tr><td>eval/f1</td><td>0.89362</td></tr><tr><td>eval/loss</td><td>0.13921</td></tr><tr><td>eval/precision</td><td>0.88369</td></tr><tr><td>eval/recall</td><td>0.90377</td></tr><tr><td>eval/runtime</td><td>8.5503</td></tr><tr><td>eval/samples_per_second</td><td>535.885</td></tr><tr><td>eval/steps_per_second</td><td>33.566</td></tr><tr><td>test/accuracy</td><td>0.97692</td></tr><tr><td>test/f1</td><td>0.89362</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">biobert-v1.1</strong> at: <a href='https://wandb.ai/250502_ohto_research/BC5/runs/a9vqksn6' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/a9vqksn6</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_133713-a9vqksn6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_134217-ek4ls09x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/ek4ls09x' target=\"_blank\">bert-base-cased</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/ek4ls09x' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/ek4ls09x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 04:32, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.104940</td>\n",
       "      <td>0.786476</td>\n",
       "      <td>0.860926</td>\n",
       "      <td>0.822019</td>\n",
       "      <td>0.964240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.096893</td>\n",
       "      <td>0.812664</td>\n",
       "      <td>0.873749</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.969265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.122100</td>\n",
       "      <td>0.118940</td>\n",
       "      <td>0.831228</td>\n",
       "      <td>0.871351</td>\n",
       "      <td>0.850817</td>\n",
       "      <td>0.969035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.138401</td>\n",
       "      <td>0.845289</td>\n",
       "      <td>0.862385</td>\n",
       "      <td>0.853752</td>\n",
       "      <td>0.970193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.030300</td>\n",
       "      <td>0.151445</td>\n",
       "      <td>0.827254</td>\n",
       "      <td>0.877189</td>\n",
       "      <td>0.851490</td>\n",
       "      <td>0.968822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.161690</td>\n",
       "      <td>0.839175</td>\n",
       "      <td>0.873645</td>\n",
       "      <td>0.856063</td>\n",
       "      <td>0.970329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.173251</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.870517</td>\n",
       "      <td>0.855489</td>\n",
       "      <td>0.970449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.186626</td>\n",
       "      <td>0.843393</td>\n",
       "      <td>0.869683</td>\n",
       "      <td>0.856336</td>\n",
       "      <td>0.970108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.186363</td>\n",
       "      <td>0.847978</td>\n",
       "      <td>0.872289</td>\n",
       "      <td>0.859962</td>\n",
       "      <td>0.971147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.188997</td>\n",
       "      <td>0.846278</td>\n",
       "      <td>0.873540</td>\n",
       "      <td>0.859693</td>\n",
       "      <td>0.970892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: answerdotai/ModernBERT-base  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 9193.50 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 10792.53 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 8363.44 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▆▆▇▆▇▇▇███</td></tr><tr><td>eval/f1</td><td>▁▅▆▇▆▇▇▇███</td></tr><tr><td>eval/loss</td><td>▂▁▃▄▅▆▇████</td></tr><tr><td>eval/precision</td><td>▁▄▆█▆▇▇▇███</td></tr><tr><td>eval/recall</td><td>▁▇▅▂█▆▅▅▆▆▆</td></tr><tr><td>eval/runtime</td><td>▆▄▇▆▆▆▃▃▃█▁</td></tr><tr><td>eval/samples_per_second</td><td>▃▅▂▃▃▃▆▆▅▁█</td></tr><tr><td>eval/steps_per_second</td><td>▃▅▂▃▃▃▆▆▅▁█</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.97089</td></tr><tr><td>eval/f1</td><td>0.85969</td></tr><tr><td>eval/loss</td><td>0.189</td></tr><tr><td>eval/precision</td><td>0.84628</td></tr><tr><td>eval/recall</td><td>0.87354</td></tr><tr><td>eval/runtime</td><td>7.5011</td></tr><tr><td>eval/samples_per_second</td><td>610.844</td></tr><tr><td>eval/steps_per_second</td><td>38.261</td></tr><tr><td>test/accuracy</td><td>0.97089</td></tr><tr><td>test/f1</td><td>0.85969</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bert-base-cased</strong> at: <a href='https://wandb.ai/250502_ohto_research/BC5/runs/ek4ls09x' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/ek4ls09x</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_134217-ek4ls09x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_134714-p5fgr9cv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/p5fgr9cv' target=\"_blank\">ModernBERT-base</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/p5fgr9cv' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/p5fgr9cv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 06:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.110805</td>\n",
       "      <td>0.760685</td>\n",
       "      <td>0.842456</td>\n",
       "      <td>0.799485</td>\n",
       "      <td>0.962311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.128122</td>\n",
       "      <td>0.843789</td>\n",
       "      <td>0.781149</td>\n",
       "      <td>0.811262</td>\n",
       "      <td>0.964048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127500</td>\n",
       "      <td>0.128068</td>\n",
       "      <td>0.811516</td>\n",
       "      <td>0.843499</td>\n",
       "      <td>0.827198</td>\n",
       "      <td>0.965156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.171176</td>\n",
       "      <td>0.858732</td>\n",
       "      <td>0.820769</td>\n",
       "      <td>0.839322</td>\n",
       "      <td>0.967617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>0.167335</td>\n",
       "      <td>0.831738</td>\n",
       "      <td>0.842665</td>\n",
       "      <td>0.837166</td>\n",
       "      <td>0.967992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.197796</td>\n",
       "      <td>0.852325</td>\n",
       "      <td>0.835262</td>\n",
       "      <td>0.843707</td>\n",
       "      <td>0.968597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.205672</td>\n",
       "      <td>0.840449</td>\n",
       "      <td>0.843603</td>\n",
       "      <td>0.842023</td>\n",
       "      <td>0.968639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.215210</td>\n",
       "      <td>0.843070</td>\n",
       "      <td>0.839641</td>\n",
       "      <td>0.841352</td>\n",
       "      <td>0.967847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.213651</td>\n",
       "      <td>0.839149</td>\n",
       "      <td>0.847461</td>\n",
       "      <td>0.843285</td>\n",
       "      <td>0.968597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.217070</td>\n",
       "      <td>0.842423</td>\n",
       "      <td>0.843916</td>\n",
       "      <td>0.843169</td>\n",
       "      <td>0.968563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: Simonlee711/Clinical_ModernBERT  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at Simonlee711/Clinical_ModernBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 10160.37 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 10529.83 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 5538.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▃▄▇▇██▇███</td></tr><tr><td>eval/f1</td><td>▁▃▅▇▇██████</td></tr><tr><td>eval/loss</td><td>▁▂▂▅▅▇▇████</td></tr><tr><td>eval/precision</td><td>▁▇▅█▆█▇▇▇▇▇</td></tr><tr><td>eval/recall</td><td>▇▁█▅▇▇█▇███</td></tr><tr><td>eval/runtime</td><td>▁▂█▂▂▄▂▃▃▂▂</td></tr><tr><td>eval/samples_per_second</td><td>█▇▁▇▇▅▇▆▆▇▇</td></tr><tr><td>eval/steps_per_second</td><td>█▇▁▇▇▅▇▆▆▇▇</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.96856</td></tr><tr><td>eval/f1</td><td>0.84317</td></tr><tr><td>eval/loss</td><td>0.21707</td></tr><tr><td>eval/precision</td><td>0.84242</td></tr><tr><td>eval/recall</td><td>0.84392</td></tr><tr><td>eval/runtime</td><td>11.7616</td></tr><tr><td>eval/samples_per_second</td><td>389.571</td></tr><tr><td>eval/steps_per_second</td><td>24.401</td></tr><tr><td>test/accuracy</td><td>0.96856</td></tr><tr><td>test/f1</td><td>0.84317</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ModernBERT-base</strong> at: <a href='https://wandb.ai/250502_ohto_research/BC5/runs/p5fgr9cv' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/p5fgr9cv</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_134714-p5fgr9cv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_135437-665gk2nh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/665gk2nh' target=\"_blank\">Clinical_ModernBERT</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/665gk2nh' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/665gk2nh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 05:24, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121918</td>\n",
       "      <td>0.795812</td>\n",
       "      <td>0.776561</td>\n",
       "      <td>0.786069</td>\n",
       "      <td>0.960531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.131500</td>\n",
       "      <td>0.809702</td>\n",
       "      <td>0.788343</td>\n",
       "      <td>0.798880</td>\n",
       "      <td>0.962532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.127600</td>\n",
       "      <td>0.146457</td>\n",
       "      <td>0.813245</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.808640</td>\n",
       "      <td>0.963001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.172387</td>\n",
       "      <td>0.809048</td>\n",
       "      <td>0.811073</td>\n",
       "      <td>0.810059</td>\n",
       "      <td>0.963614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.018100</td>\n",
       "      <td>0.182775</td>\n",
       "      <td>0.810892</td>\n",
       "      <td>0.818163</td>\n",
       "      <td>0.814511</td>\n",
       "      <td>0.964670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.189245</td>\n",
       "      <td>0.806652</td>\n",
       "      <td>0.824314</td>\n",
       "      <td>0.815388</td>\n",
       "      <td>0.964347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.209020</td>\n",
       "      <td>0.801517</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.813617</td>\n",
       "      <td>0.964568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.214871</td>\n",
       "      <td>0.802693</td>\n",
       "      <td>0.826713</td>\n",
       "      <td>0.814526</td>\n",
       "      <td>0.964415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.224123</td>\n",
       "      <td>0.814765</td>\n",
       "      <td>0.815869</td>\n",
       "      <td>0.815316</td>\n",
       "      <td>0.964696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.225102</td>\n",
       "      <td>0.812163</td>\n",
       "      <td>0.817329</td>\n",
       "      <td>0.814738</td>\n",
       "      <td>0.964594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: thomas-sounack/BioClinical-ModernBERT-base  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForTokenClassification were not initialized from the model checkpoint at thomas-sounack/BioClinical-ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 4561/4561 [00:00<00:00, 8558.19 examples/s]\n",
      "Map: 100%|██████████| 4582/4582 [00:00<00:00, 10309.45 examples/s]\n",
      "Map: 100%|██████████| 4798/4798 [00:00<00:00, 8699.81 examples/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>▁▄▅▆█▇█████</td></tr><tr><td>eval/f1</td><td>▁▄▆▇███████</td></tr><tr><td>eval/loss</td><td>▁▂▃▄▅▆▇▇███</td></tr><tr><td>eval/precision</td><td>▁▆▇▆▇▅▃▄█▇▇</td></tr><tr><td>eval/recall</td><td>▁▃▅▆▇███▆▇▇</td></tr><tr><td>eval/runtime</td><td>▁▄▂▆▂▁█▄▃▅▁</td></tr><tr><td>eval/samples_per_second</td><td>█▅▇▃▇█▁▅▆▄█</td></tr><tr><td>eval/steps_per_second</td><td>█▅▇▃▇█▁▅▆▄█</td></tr><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/f1</td><td>▁</td></tr><tr><td>+11</td><td>...</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/accuracy</td><td>0.96459</td></tr><tr><td>eval/f1</td><td>0.81474</td></tr><tr><td>eval/loss</td><td>0.2251</td></tr><tr><td>eval/precision</td><td>0.81216</td></tr><tr><td>eval/recall</td><td>0.81733</td></tr><tr><td>eval/runtime</td><td>8.4817</td></tr><tr><td>eval/samples_per_second</td><td>540.222</td></tr><tr><td>eval/steps_per_second</td><td>33.838</td></tr><tr><td>test/accuracy</td><td>0.96459</td></tr><tr><td>test/f1</td><td>0.81474</td></tr><tr><td>+16</td><td>...</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">Clinical_ModernBERT</strong> at: <a href='https://wandb.ai/250502_ohto_research/BC5/runs/665gk2nh' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/665gk2nh</a><br> View project at: <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251129_135437-665gk2nh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/1-project/1-nlp/github/nlp-util/benchmark_test/notebook/wandb/run-20251129_140028-0pkgcbwe</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/250502_ohto_research/BC5/runs/0pkgcbwe' target=\"_blank\">BioClinical-ModernBERT-base</a></strong> to <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/250502_ohto_research/BC5' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/250502_ohto_research/BC5/runs/0pkgcbwe' target=\"_blank\">https://wandb.ai/250502_ohto_research/BC5/runs/0pkgcbwe</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_843539/1228510784.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2860' max='2860' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2860/2860 06:44, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.092396</td>\n",
       "      <td>0.813974</td>\n",
       "      <td>0.878219</td>\n",
       "      <td>0.844877</td>\n",
       "      <td>0.968529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.105089</td>\n",
       "      <td>0.859606</td>\n",
       "      <td>0.810760</td>\n",
       "      <td>0.834469</td>\n",
       "      <td>0.969397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.112331</td>\n",
       "      <td>0.841091</td>\n",
       "      <td>0.864769</td>\n",
       "      <td>0.852766</td>\n",
       "      <td>0.971918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.135920</td>\n",
       "      <td>0.859498</td>\n",
       "      <td>0.875091</td>\n",
       "      <td>0.867225</td>\n",
       "      <td>0.973835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.017200</td>\n",
       "      <td>0.151826</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.877698</td>\n",
       "      <td>0.866674</td>\n",
       "      <td>0.973562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.169289</td>\n",
       "      <td>0.868666</td>\n",
       "      <td>0.866854</td>\n",
       "      <td>0.867759</td>\n",
       "      <td>0.973971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.165084</td>\n",
       "      <td>0.852307</td>\n",
       "      <td>0.882077</td>\n",
       "      <td>0.866937</td>\n",
       "      <td>0.973920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.174699</td>\n",
       "      <td>0.862749</td>\n",
       "      <td>0.875613</td>\n",
       "      <td>0.869133</td>\n",
       "      <td>0.974065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.178495</td>\n",
       "      <td>0.861598</td>\n",
       "      <td>0.875613</td>\n",
       "      <td>0.868549</td>\n",
       "      <td>0.973980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.179900</td>\n",
       "      <td>0.861850</td>\n",
       "      <td>0.875508</td>\n",
       "      <td>0.868625</td>\n",
       "      <td>0.974014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i, name in enumerate(models):\n",
    "    print(\"=== Model:\", name, \" Training ===\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "    tokenizer.pad_token = \"[PAD]\" \n",
    "\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "    name,\n",
    "    num_labels=len(label_names),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    tokenized_datasets = dataset.map(\n",
    "        lambda x: tokenize_and_align_labels(x, tokenizer),\n",
    "        batched=True\n",
    "    )\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    model_name = name.split(\"/\")[-1]\n",
    "    \n",
    "    wandb.init(\n",
    "    entity=\"250502_ohto_research\",\n",
    "    project=\"BC5\", name=model_name, \n",
    "    config={\n",
    "        \"model_name\": model_name,\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"num_epochs\": 10,\n",
    "        \"dataset\": \"BC5\",\n",
    "    })\n",
    "\n",
    "    os.makedirs(f\"../result/bc5/{model_name}\", exist_ok=True)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"../result/bc5/{model_name}\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=10,\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "    df_final_results = result_output_general(\n",
    "        trainer=trainer,\n",
    "        tokenized_datasets=tokenized_datasets,\n",
    "        tokenizer=tokenizer,\n",
    "        id2label=id2label,\n",
    "        num_samples_to_process=len(tokenized_datasets[\"validation\"]),\n",
    "        output_filename=f'../result/bc5/{model_name}/results.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce1ea53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
