{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6fa05b3",
   "metadata": {},
   "source": [
    " * @ Author: Yohei Ohto\n",
    " * @ Create Time: 2025-12-08 11:35:29\n",
    " * @ Modified time: 2025-12-08 11:37:25\n",
    " * @ Description: 既存MLMへのsequence classificationタスクの追加実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263de366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/test_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from sklearn.manifold import TSNE\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import (AutoModel, AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer, DataCollatorWithPadding, Trainer, DataCollatorForTokenClassification,\n",
    "                          TrainingArguments)\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee1e1f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    \"YoheiOhto/251225100096_model\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a189121",
   "metadata": {},
   "source": [
    "# get raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923f60e3",
   "metadata": {},
   "source": [
    "これを参考にして、BLURBのベンチマークを取る https://github.com/michiyasunaga/LinkBERT/tree/main/scripts    \n",
    "hugging faceだと整備しきってない感  \n",
    "dataはここから取れる https://nlp.stanford.edu/projects/myasu/LinkBERT/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bfde265",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"../data/raw/data.zip\"\n",
    "\n",
    "import zipfile\n",
    "os.makedirs(\"../data/defreeze\", exist_ok=True)\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"../data/defreeze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6165c2ba",
   "metadata": {},
   "source": [
    "## HOC以外は実行できる  \n",
    " ---  \n",
    "* seqcls/DDI_hf 'sentence' & 1 label prediction  \n",
    "* seqcls/chemprot_hf 'sentence' & 1 label prediction  \n",
    "* seqcls/GAD_hf 'sentence' & 1 label prediction  \n",
    " ---\n",
    "* seqcls/bioasq_hf 'sentence1', 'sentence2' & 1 label prediction\n",
    "* seqcls/pubmedqa_hf 'sentence1', 'sentence2' & 1 label prediction  \n",
    " ---\n",
    "* seqcls/hoc_hf 'sentence' & multi-label prediction  \n",
    "* seqcls/HoC_hf 'sentence' & multi-label prediction  \n",
    " ---\n",
    "* seqcls/BIOSSES_hf 'sentence1', 'sentence2' & regression prediction  \n",
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338d9e8",
   "metadata": {},
   "source": [
    "# DDI, chemprot, GAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8d62ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90cb679",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "    f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"precision\"]\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")[\"recall\"]\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"f1\": f1,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea8c8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    result = tokenizer(examples['sentence'], truncation=True, max_length=256)\n",
    "    result[\"label\"] = [label2id[l] for l in examples[\"label\"]]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3f3000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: YoheiOhto/251225100096_model  Data: DDI  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at YoheiOhto/251225100096_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 2496/2496 [00:00<00:00, 11709.67 examples/s]\n",
      "/tmp/ipykernel_286754/2948749809.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None, 'pad_token_id': 0}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15810' max='15810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15810/15810 2:57:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.131200</td>\n",
       "      <td>0.223788</td>\n",
       "      <td>0.949920</td>\n",
       "      <td>0.831689</td>\n",
       "      <td>0.901655</td>\n",
       "      <td>0.808193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.053000</td>\n",
       "      <td>0.246672</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.811620</td>\n",
       "      <td>0.929172</td>\n",
       "      <td>0.763854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.031800</td>\n",
       "      <td>0.285444</td>\n",
       "      <td>0.953926</td>\n",
       "      <td>0.796742</td>\n",
       "      <td>0.844644</td>\n",
       "      <td>0.768714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.017800</td>\n",
       "      <td>0.342526</td>\n",
       "      <td>0.953526</td>\n",
       "      <td>0.829170</td>\n",
       "      <td>0.831729</td>\n",
       "      <td>0.830350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.012400</td>\n",
       "      <td>0.376700</td>\n",
       "      <td>0.950721</td>\n",
       "      <td>0.782726</td>\n",
       "      <td>0.809041</td>\n",
       "      <td>0.760905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.438698</td>\n",
       "      <td>0.947917</td>\n",
       "      <td>0.807048</td>\n",
       "      <td>0.809626</td>\n",
       "      <td>0.808306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>0.421532</td>\n",
       "      <td>0.951122</td>\n",
       "      <td>0.790325</td>\n",
       "      <td>0.818376</td>\n",
       "      <td>0.768967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.439227</td>\n",
       "      <td>0.952324</td>\n",
       "      <td>0.798873</td>\n",
       "      <td>0.809257</td>\n",
       "      <td>0.790769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.467989</td>\n",
       "      <td>0.951522</td>\n",
       "      <td>0.803925</td>\n",
       "      <td>0.840055</td>\n",
       "      <td>0.774440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.456337</td>\n",
       "      <td>0.952724</td>\n",
       "      <td>0.800190</td>\n",
       "      <td>0.819478</td>\n",
       "      <td>0.784847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/result/DDI/251225100096_model/all_results.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     44\u001b[39m trainer.train()\n\u001b[32m     45\u001b[39m metrics = trainer.evaluate()\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m df_results = result_output_seq_classification(\n\u001b[32m     49\u001b[39m     trainer,\n\u001b[32m     50\u001b[39m     tokenized_datasets,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m     output_filename=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m../data/result/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_name.lower()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_results.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     54\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/test_env/lib/python3.12/site-packages/transformers/trainer_pt_utils.py:1070\u001b[39m, in \u001b[36msave_metrics\u001b[39m\u001b[34m(self, split, metrics, combined)\u001b[39m\n\u001b[32m   1067\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1069\u001b[39m path = os.path.join(\u001b[38;5;28mself\u001b[39m.args.output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_results.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mw\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m   1071\u001b[39m     json.dump(metrics, f, indent=\u001b[32m4\u001b[39m, sort_keys=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m combined:\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/result/DDI/251225100096_model/all_results.json'"
     ]
    }
   ],
   "source": [
    "for data_name in [\"DDI\", \"chemprot\", \"GAD\"]:\n",
    "    data = load_dataset(f\"../data/defreeze/data/seqcls/{data_name}_hf\")\n",
    "\n",
    "    label_list = sorted(data['train'].unique('label'))\n",
    "    num_labels = len(label_list)\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    for i, name in enumerate(models):\n",
    "        print(\"=== Model:\", name, \" Data:\", data_name, \" Training ===\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        tokenizer.pad_token = \"[PAD]\" \n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "        model_name = name.split(\"/\")[-1]\n",
    "\n",
    "        wandb.init(\n",
    "        entity=\"250502_ohto_research\",\n",
    "        project=data_name, name=model_name, \n",
    "        config={\n",
    "            \"model_name\": model_name,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"num_epochs\": 10,\n",
    "            \"dataset\": data_name,\n",
    "        })\n",
    "\n",
    "        tokenized_datasets = data.map(preprocess_function, batched=True)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"../data/result/{data_name}/{model_name}\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=10,\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "        df_results = result_output_seq_classification(\n",
    "            trainer,\n",
    "            tokenized_datasets,\n",
    "            tokenizer,\n",
    "            id2label,\n",
    "            output_filename=f\"../data/result/{data_name}/{model_name}/{data_name.lower()}_results.csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9425fa18",
   "metadata": {},
   "source": [
    "# bioasq, pubmedqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24d7ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    if \"sentence1\" in examples and \"sentence2\" in examples:\n",
    "        result = tokenizer(\n",
    "            examples[\"sentence1\"], \n",
    "            examples[\"sentence2\"], \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        )\n",
    "    elif \"sentence\" in examples:\n",
    "        result = tokenizer(\n",
    "            examples[\"sentence\"], \n",
    "            truncation=True, \n",
    "            max_length=256\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"Input text fields (sentence or sentence1/2) not found.\")\n",
    "\n",
    "    if \"label\" in examples and len(examples[\"label\"]) > 0:\n",
    "        first_label = examples[\"label\"][0]\n",
    "\n",
    "        if isinstance(first_label, float):\n",
    "            result[\"label\"] = examples[\"label\"]\n",
    "\n",
    "        elif isinstance(first_label, list):\n",
    "            result[\"label\"] = [[float(l) for l in labels] for labels in examples[\"label\"]]\n",
    "\n",
    "        else:\n",
    "            processed_labels = []\n",
    "            for l in examples[\"label\"]:\n",
    "                if 'label2id' in globals() and l in label2id:\n",
    "                    processed_labels.append(label2id[l])\n",
    "                elif isinstance(l, str) and l.isdigit():\n",
    "                    processed_labels.append(int(l))\n",
    "                elif isinstance(l, int):\n",
    "                    processed_labels.append(l)\n",
    "                else:\n",
    "                    processed_labels.append(l)\n",
    "            \n",
    "            result[\"label\"] = processed_labels\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "892ff9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, r2_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    if labels.dtype == np.float32 or labels.dtype == np.float64:\n",
    "        predictions = np.squeeze(predictions)\n",
    "        labels = np.squeeze(labels)\n",
    "        \n",
    "        mse = mean_squared_error(labels, predictions)\n",
    "        pearson_corr, _ = pearsonr(labels, predictions)\n",
    "        \n",
    "        return {\n",
    "            \"mse\": mse,\n",
    "            \"pearson\": pearson_corr\n",
    "        }\n",
    "\n",
    "    elif len(labels.shape) > 1 and labels.shape[1] > 1:\n",
    "        probs = 1 / (1 + np.exp(-predictions))\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        \n",
    "        f1_micro = f1_score(y_true=labels, y_pred=preds, average='micro')\n",
    "        f1_macro = f1_score(y_true=labels, y_pred=preds, average='macro')\n",
    "        accuracy = accuracy_score(y_true=labels, y_pred=preds)\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_micro\": f1_micro,\n",
    "            \"f1_macro\": f1_macro\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        accuracy = accuracy_score(labels, preds)\n",
    "        f1 = f1_score(labels, preds, average=\"macro\")\n",
    "        precision = precision_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(labels, preds, average=\"macro\", zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3764682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def result_output_unified(\n",
    "    trainer,\n",
    "    tokenized_datasets,\n",
    "    tokenizer,\n",
    "    id2label=None,\n",
    "    target_split=\"validation\",\n",
    "    output_filename='prediction_results.csv'\n",
    "):\n",
    "    print(f\"Generating predictions for {target_split} set...\")\n",
    "    \n",
    "    predictions, labels, _ = trainer.predict(tokenized_datasets[target_split])\n",
    "\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "\n",
    "    is_regression = (id2label is None) or (labels.dtype == np.float32) or (labels.dtype == np.float64)\n",
    "    \n",
    "    is_multilabel = (not is_regression) and (len(labels.shape) > 1 and labels.shape[1] > 1)\n",
    "\n",
    "    is_singlelabel = (not is_regression) and (not is_multilabel)\n",
    "\n",
    "    all_results_list = []\n",
    "    num_samples = len(tokenized_datasets[target_split])\n",
    "\n",
    "    if is_regression:\n",
    "        pred_scores = np.squeeze(predictions)\n",
    "        true_scores = np.squeeze(labels)\n",
    "    elif is_multilabel:\n",
    "        probs = torch.sigmoid(torch.from_numpy(predictions)).numpy()\n",
    "    else:\n",
    "        probs = torch.nn.functional.softmax(torch.from_numpy(predictions), dim=1).numpy()\n",
    "        pred_ids = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(f\"Task Type Detected: {'Regression' if is_regression else 'Multi-label' if is_multilabel else 'Single-label'}\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_ids = tokenized_datasets[target_split][i][\"input_ids\"]\n",
    "        text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "        \n",
    "        row_data = {'Text': text}\n",
    "\n",
    "        if is_regression:\n",
    "            pred_val = pred_scores[i]\n",
    "            true_val = true_scores[i]\n",
    "            row_data['Predicted'] = float(pred_val)\n",
    "            row_data['True_Label'] = float(true_val)\n",
    "            row_data['Diff'] = abs(pred_val - true_val) \n",
    "\n",
    "        elif is_multilabel:\n",
    "            active_preds = np.where(probs[i] > 0.5)[0]\n",
    "            active_trues = np.where(labels[i] == 1)[0]\n",
    "            \n",
    "            pred_names = [id2label[idx] for idx in active_preds]\n",
    "            true_names = [id2label[idx] for idx in active_trues]\n",
    "            \n",
    "            row_data['Predicted'] = \"; \".join(pred_names)\n",
    "            row_data['True_Label'] = \"; \".join(true_names)\n",
    "            \n",
    "            row_data['Is_Perfect_Match'] = (set(pred_names) == set(true_names))\n",
    "            \n",
    "            for idx, label_name in id2label.items():\n",
    "                row_data[f'P({label_name})'] = probs[i][idx]\n",
    "\n",
    "        else:\n",
    "            true_id = labels[i]\n",
    "            pred_id = pred_ids[i]\n",
    "            \n",
    "            true_name = id2label.get(true_id, str(true_id))\n",
    "            pred_name = id2label.get(pred_id, str(pred_id))\n",
    "            \n",
    "            row_data['Predicted'] = pred_name\n",
    "            row_data['True_Label'] = true_name\n",
    "            row_data['Is_Correct'] = (pred_name == true_name)\n",
    "            \n",
    "            row_data['Confidence'] = probs[i][pred_id] \n",
    "\n",
    "            for label_id, label_name in id2label.items():\n",
    "               row_data[f'P({label_name})'] = probs[i][label_id]\n",
    "\n",
    "        all_results_list.append(row_data)\n",
    "\n",
    "    df = pd.DataFrame(all_results_list)\n",
    "    os.makedirs(os.path.dirname(output_filename), exist_ok=True)\n",
    "    df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"Results saved to {output_filename}\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f065cdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 450 examples [00:00, 258872.14 examples/s]\n",
      "Generating validation split: 50 examples [00:00, 63148.21 examples/s]\n",
      "Generating test split: 500 examples [00:00, 316169.46 examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model: microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 1801.36 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1465.12 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2310.51 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 00:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.948130</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973132</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.995163</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.269697</td>\n",
       "      <td>0.256039</td>\n",
       "      <td>0.299927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.013896</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.242137</td>\n",
       "      <td>0.227778</td>\n",
       "      <td>0.258533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.056240</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.353865</td>\n",
       "      <td>0.343520</td>\n",
       "      <td>0.410312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.019204</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.306536</td>\n",
       "      <td>0.289364</td>\n",
       "      <td>0.327524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.128141</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.340477</td>\n",
       "      <td>0.321581</td>\n",
       "      <td>0.376180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.147235</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.312623</td>\n",
       "      <td>0.297619</td>\n",
       "      <td>0.351489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.181477</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.309716</td>\n",
       "      <td>0.363834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.193448</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.326667</td>\n",
       "      <td>0.309716</td>\n",
       "      <td>0.363834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext/pubmedqa_results.csv\n",
      "=== Model: dmis-lab/biobert-v1.1  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2708.34 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 2054.38 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1473.11 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 00:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.955573</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.960872</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.991807</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.244948</td>\n",
       "      <td>0.229798</td>\n",
       "      <td>0.303558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.057189</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.256209</td>\n",
       "      <td>0.240048</td>\n",
       "      <td>0.275962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.210974</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.210909</td>\n",
       "      <td>0.204509</td>\n",
       "      <td>0.228758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.377561</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.266199</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.340595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.477126</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.245682</td>\n",
       "      <td>0.230324</td>\n",
       "      <td>0.263617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.672839</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.215335</td>\n",
       "      <td>0.201389</td>\n",
       "      <td>0.231663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.754340</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.231944</td>\n",
       "      <td>0.213444</td>\n",
       "      <td>0.261438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.778852</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.222905</td>\n",
       "      <td>0.205026</td>\n",
       "      <td>0.249092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/biobert-v1.1/pubmedqa_results.csv\n",
      "=== Model: google-bert/bert-base-cased  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2778.68 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 1258.00 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2458.68 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 00:52, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.947539</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.956671</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.962657</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.997800</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.176871</td>\n",
       "      <td>0.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.055697</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.254669</td>\n",
       "      <td>0.287582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.288177</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.288671</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.248388</td>\n",
       "      <td>0.233333</td>\n",
       "      <td>0.286129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.481870</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.235558</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.291213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.521832</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.241302</td>\n",
       "      <td>0.229060</td>\n",
       "      <td>0.273784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.581801</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.256232</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.298475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/bert-base-cased/pubmedqa_results.csv\n",
      "=== Model: answerdotai/ModernBERT-base  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at answerdotai/ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2010.24 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 2398.36 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1640.74 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 03:51, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.984101</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260057</td>\n",
       "      <td>0.264493</td>\n",
       "      <td>0.328250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.058503</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.172644</td>\n",
       "      <td>0.360000</td>\n",
       "      <td>0.252186</td>\n",
       "      <td>0.268519</td>\n",
       "      <td>0.302106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.158278</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.260057</td>\n",
       "      <td>0.264493</td>\n",
       "      <td>0.328250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.465195</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.374074</td>\n",
       "      <td>0.378195</td>\n",
       "      <td>0.484749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.421444</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.405861</td>\n",
       "      <td>0.638756</td>\n",
       "      <td>0.393246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.786125</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.355120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.114757</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.315217</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.355120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.439806</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.315528</td>\n",
       "      <td>0.352713</td>\n",
       "      <td>0.355120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.431877</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.292308</td>\n",
       "      <td>0.295322</td>\n",
       "      <td>0.318083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/ModernBERT-base/pubmedqa_results.csv\n",
      "=== Model: Simonlee711/Clinical_ModernBERT  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Simonlee711/Clinical_ModernBERT and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 2195.48 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 2284.13 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 2472.76 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 01:02, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.980652</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.004583</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.273879</td>\n",
       "      <td>0.517007</td>\n",
       "      <td>0.352941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.220083</td>\n",
       "      <td>0.340000</td>\n",
       "      <td>0.240967</td>\n",
       "      <td>0.231602</td>\n",
       "      <td>0.267974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.314234</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.265006</td>\n",
       "      <td>0.249010</td>\n",
       "      <td>0.283224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.672227</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.393432</td>\n",
       "      <td>0.616487</td>\n",
       "      <td>0.370733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.999494</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.338790</td>\n",
       "      <td>0.366487</td>\n",
       "      <td>0.331518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.073219</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.365281</td>\n",
       "      <td>0.436749</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.143933</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.365281</td>\n",
       "      <td>0.436749</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.191538</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.365281</td>\n",
       "      <td>0.436749</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.207464</td>\n",
       "      <td>0.440000</td>\n",
       "      <td>0.365281</td>\n",
       "      <td>0.436749</td>\n",
       "      <td>0.351126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/Clinical_ModernBERT/pubmedqa_results.csv\n",
      "=== Model: thomas-sounack/BioClinical-ModernBERT-base  Data: pubmedqa  Training ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ModernBertForSequenceClassification were not initialized from the model checkpoint at thomas-sounack/BioClinical-ModernBERT-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 450/450 [00:00<00:00, 1572.62 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 487.97 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1839.69 examples/s]\n",
      "/tmp/ipykernel_761457/1229339976.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': None, 'bos_token_id': None}.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='290' max='290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [290/290 03:50, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.980602</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.228070</td>\n",
       "      <td>0.176871</td>\n",
       "      <td>0.320988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.059930</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.233766</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.202001</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.297808</td>\n",
       "      <td>0.299769</td>\n",
       "      <td>0.339143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.119106</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.270531</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.310821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.212800</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.386667</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.409586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.364120</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.374062</td>\n",
       "      <td>0.603659</td>\n",
       "      <td>0.378722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.595955</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.438672</td>\n",
       "      <td>0.660354</td>\n",
       "      <td>0.420116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.835409</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.340584</td>\n",
       "      <td>0.331871</td>\n",
       "      <td>0.369644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.833256</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.340584</td>\n",
       "      <td>0.331871</td>\n",
       "      <td>0.369644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.892775</td>\n",
       "      <td>0.540000</td>\n",
       "      <td>0.340584</td>\n",
       "      <td>0.331871</td>\n",
       "      <td>0.369644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for validation set...\n",
      "Results saved to ../data/result/pubmedqa/BioClinical-ModernBERT-base/pubmedqa_results.csv\n"
     ]
    }
   ],
   "source": [
    "for data_name in [\"pubmedqa\"]:\n",
    "    data = load_dataset(f\"../data/defreeze/data/seqcls/{data_name}_hf\")\n",
    "\n",
    "    label_list = sorted(data['train'].unique('label'))\n",
    "    num_labels = len(label_list)\n",
    "    id2label = {i: label for i, label in enumerate(label_list)}\n",
    "    label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    for i, name in enumerate(models):\n",
    "        print(\"=== Model:\", name, \" Data:\", data_name, \" Training ===\")\n",
    "\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        tokenizer.pad_token = \"[PAD]\" \n",
    "\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        name, num_labels=num_labels, id2label=id2label, label2id=label2id)\n",
    "\n",
    "        model_name = name.split(\"/\")[-1]\n",
    "        wandb.init(\n",
    "        entity=\"250502_ohto_research\",\n",
    "        project=data_name, name=model_name, \n",
    "        config={\n",
    "            \"model_name\": model_name,\n",
    "            \"learning_rate\": 2e-5,\n",
    "            \"batch_size\": 16,\n",
    "            \"num_epochs\": 10,\n",
    "            \"dataset\": data_name,\n",
    "        })\n",
    "        tokenized_datasets = data.map(preprocess_function, batched=True)\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=f\"../data/result/{data_name}/{model_name}\",\n",
    "            learning_rate=2e-5,\n",
    "            per_device_train_batch_size=16,\n",
    "            per_device_eval_batch_size=16,\n",
    "            num_train_epochs=10,\n",
    "            save_strategy=\"no\",\n",
    "            load_best_model_at_end=False,\n",
    "            weight_decay=0.01,\n",
    "            eval_strategy=\"epoch\",\n",
    "            report_to=\"wandb\",\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"validation\"],\n",
    "            tokenizer=tokenizer,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        metrics = trainer.evaluate()\n",
    "        trainer.save_metrics(\"all\", metrics)\n",
    "\n",
    "        df_results = result_output_seq_classification(\n",
    "            trainer,\n",
    "            tokenized_datasets,\n",
    "            tokenizer,\n",
    "            id2label,\n",
    "            output_filename=f\"../data/result/{data_name}/{model_name}/{data_name.lower()}_results.csv\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fef440d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HoC'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "test_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
